{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0db3f1",
   "metadata": {},
   "source": [
    "# Neural Networks - 12 Practicals for Colab/Jupyter (Simple Version)\n",
    "\n",
    "*Setup: Run this first*\n",
    "python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Simple Perceptron (NumPy only)\n",
    "\n",
    "python\n",
    "import numpy as np\n",
    "\n",
    "# Initialize\n",
    "weights = np.random.randn(2)\n",
    "bias = 0\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Activation function\n",
    "def predict(x):\n",
    "    output = np.dot(x, weights) + bias\n",
    "    return 1 if output >= 0 else 0\n",
    "\n",
    "# Training data (AND gate)\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Train\n",
    "for epoch in range(100):\n",
    "    for i in range(len(X)):\n",
    "        prediction = predict(X[i])\n",
    "        error = y[i] - prediction\n",
    "        \n",
    "        # Update weights\n",
    "        weights = weights + learning_rate * error * X[i]\n",
    "        bias = bias + learning_rate * error\n",
    "\n",
    "# Test\n",
    "print(\"AND Gate Results:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"{X[i]} -> {predict(X[i])}\")\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Multi-Layer Perceptron (NumPy only)\n",
    "\n",
    "python\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Initialize weights\n",
    "w1 = np.random.randn(2, 4) * 0.1  # Input to hidden\n",
    "b1 = np.zeros((1, 4))\n",
    "w2 = np.random.randn(4, 1) * 0.1  # Hidden to output\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "# Forward pass\n",
    "X = np.array([[0.5, 0.3]])\n",
    "\n",
    "# Layer 1\n",
    "z1 = np.dot(X, w1) + b1\n",
    "a1 = sigmoid(z1)\n",
    "\n",
    "# Layer 2\n",
    "z2 = np.dot(a1, w2) + b2\n",
    "output = sigmoid(z2)\n",
    "\n",
    "print(\"Output:\", output)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. MLP with Backpropagation (NumPy only)\n",
    "\n",
    "python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training data (XOR)\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize weights\n",
    "w1 = np.random.randn(2, 4) * 0.5\n",
    "b1 = np.zeros((1, 4))\n",
    "w2 = np.random.randn(4, 1) * 0.5\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    # Calculate error\n",
    "    error = a2 - y\n",
    "    \n",
    "    # Backpropagation\n",
    "    delta2 = error * a2 * (1 - a2)\n",
    "    delta1 = np.dot(delta2, w2.T) * a1 * (1 - a1)\n",
    "    \n",
    "    # Update weights\n",
    "    w2 = w2 - learning_rate * np.dot(a1.T, delta2)\n",
    "    b2 = b2 - learning_rate * np.sum(delta2, axis=0)\n",
    "    w1 = w1 - learning_rate * np.dot(X.T, delta1)\n",
    "    b1 = b1 - learning_rate * np.sum(delta1, axis=0)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean((a2 - y) ** 2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test\n",
    "print(\"\\nXOR Results:\")\n",
    "for i in range(len(X)):\n",
    "    z1 = np.dot(X[i], w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    prediction = sigmoid(z2)\n",
    "    print(f\"{X[i]} -> {prediction[0][0]:.4f}\")\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. CNN with NumPy only\n",
    "\n",
    "python\n",
    "import numpy as np\n",
    "\n",
    "# Convolution\n",
    "def conv2d(image, kernel):\n",
    "    h, w = image.shape\n",
    "    kh, kw = kernel.shape\n",
    "    output = np.zeros((h-kh+1, w-kw+1))\n",
    "    \n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            output[i,j] = np.sum(image[i:i+kh, j:j+kw] * kernel)\n",
    "    return output\n",
    "\n",
    "# ReLU activation\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Max pooling\n",
    "def max_pool(image, size=2):\n",
    "    h, w = image.shape\n",
    "    output = np.zeros((h//size, w//size))\n",
    "    \n",
    "    for i in range(0, h, size):\n",
    "        for j in range(0, w, size):\n",
    "            output[i//size, j//size] = np.max(image[i:i+size, j:j+size])\n",
    "    return output\n",
    "\n",
    "# Test\n",
    "image = np.random.randn(28, 28)\n",
    "kernel = np.random.randn(3, 3)\n",
    "\n",
    "conv_output = conv2d(image, kernel)\n",
    "activated = relu(conv_output)\n",
    "pooled = max_pool(activated)\n",
    "\n",
    "print(f\"Image: {image.shape}\")\n",
    "print(f\"After convolution: {conv_output.shape}\")\n",
    "print(f\"After pooling: {pooled.shape}\")\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5. CNN with TensorFlow\n",
    "\n",
    "python\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "# Build CNN\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Different Optimizers\n",
    "\n",
    "python\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784) / 255.0\n",
    "X_test = X_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "# --- Model with SGD ---\n",
    "model1 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model1.compile(optimizer=SGD(learning_rate=0.01),\n",
    "               loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "print(\"Training with SGD...\")\n",
    "model1.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "# --- Model with Adam ---\n",
    "model2 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model2.compile(optimizer=Adam(learning_rate=0.001),\n",
    "               loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "print(\"\\nTraining with Adam...\")\n",
    "model2.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "# --- Model with RMSprop ---\n",
    "model3 = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model3.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "               loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "print(\"\\nTraining with RMSprop...\")\n",
    "model3.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Transfer Learning\n",
    "\n",
    "python\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load pre-trained VGG16\n",
    "base_model = VGG16(weights='imagenet',\n",
    "                   include_top=False,\n",
    "                   input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add new layers\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "# model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "print(\"Model created with pre-trained VGG16!\")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 8. DCGAN\n",
    "\n",
    "python\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Generator model\n",
    "generator = models.Sequential([\n",
    "    layers.Dense(7*7*256, input_dim=100),\n",
    "    layers.Reshape((7, 7, 256)),\n",
    "    layers.Conv2DTranspose(128, 5, strides=2, padding='same'),\n",
    "    layers.LeakyReLU(0.2),\n",
    "    layers.Conv2DTranspose(64, 5, strides=2, padding='same'),\n",
    "    layers.LeakyReLU(0.2),\n",
    "    layers.Conv2DTranspose(1, 5, strides=2, padding='same', activation='tanh')\n",
    "])\n",
    "\n",
    "# Discriminator model\n",
    "discriminator = models.Sequential([\n",
    "    layers.Conv2D(64, 5, strides=2, padding='same', input_shape=(28,28,1)),\n",
    "    layers.LeakyReLU(0.2),\n",
    "    layers.Conv2D(128, 5, strides=2, padding='same'),\n",
    "    layers.LeakyReLU(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Combined GAN\n",
    "discriminator.trainable = False\n",
    "gan = models.Sequential([generator, discriminator])\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "print(\"GAN models created!\")\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Autoencoder\n",
    "\n",
    "python\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Encoder\n",
    "encoder = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu')\n",
    "])\n",
    "\n",
    "# Decoder\n",
    "decoder = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(784, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Complete autoencoder\n",
    "autoencoder = models.Sequential([encoder, decoder])\n",
    "\n",
    "# Compile\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Load and prepare data\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, _), (X_test, _) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784) / 255.0\n",
    "X_test = X_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "# Train (input = output)\n",
    "autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, validation_data=(X_test, X_test))\n",
    "\n",
    "# Test\n",
    "compressed = encoder.predict(X_test[:10])\n",
    "reconstructed = decoder.predict(compressed)\n",
    "print(\"Compressed shape:\", compressed.shape)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Variational Autoencoder (VAE)\n",
    "\n",
    "python\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Encoder\n",
    "encoder_input = layers.Input(shape=(784,))\n",
    "x = layers.Dense(256, activation='relu')(encoder_input)\n",
    "z_mean = layers.Dense(2)(x)\n",
    "z_log_var = layers.Dense(2)(x)\n",
    "\n",
    "# Sampling\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_input = layers.Input(shape=(2,))\n",
    "x = layers.Dense(256, activation='relu')(decoder_input)\n",
    "decoder_output = layers.Dense(784, activation='sigmoid')(x)\n",
    "\n",
    "# Models\n",
    "encoder = models.Model(encoder_input, [z_mean, z_log_var, z])\n",
    "decoder = models.Model(decoder_input, decoder_output)\n",
    "\n",
    "# VAE\n",
    "vae_output = decoder(z)\n",
    "vae = models.Model(encoder_input, vae_output)\n",
    "\n",
    "# Loss\n",
    "reconstruction_loss = tf.reduce_mean(tf.reduce_sum(\n",
    "    tf.keras.losses.binary_crossentropy(encoder_input, vae_output), axis=1))\n",
    "kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "vae_loss = reconstruction_loss + kl_loss\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "# Compile\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# Load data\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, _), _ = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784) / 255.0\n",
    "\n",
    "# Train\n",
    "vae.fit(X_train, epochs=10, batch_size=128)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 11. LSTM\n",
    "\n",
    "python\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Example 1: Simple LSTM for sequences\n",
    "model = models.Sequential([\n",
    "    layers.LSTM(64, input_shape=(10, 1)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Example 2: LSTM for text classification\n",
    "model_text = models.Sequential([\n",
    "    layers.Embedding(10000, 128, input_length=100),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_text.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create sample time series data\n",
    "data = np.sin(np.linspace(0, 100, 1000))\n",
    "X, y = [], []\n",
    "for i in range(len(data) - 10):\n",
    "    X.append(data[i:i+10])\n",
    "    y.append(data[i+10])\n",
    "X = np.array(X).reshape(-1, 10, 1)\n",
    "y = np.array(y)\n",
    "\n",
    "# Train\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "print(\"LSTM trained!\")\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 12. BiLSTM\n",
    "\n",
    "python\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# BiLSTM for text classification\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(10000, 128, input_length=100),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(32)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train with your data\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Tips for Colab/Jupyter:\n",
    "\n",
    "1. *Run cells one by one* - Don't run all at once\n",
    "2. *Check GPU*: Runtime → Change runtime type → GPU\n",
    "3. *Save work*: File → Download → .ipynb\n",
    "4. *Clear output*: Edit → Clear all outputs (before submitting)\n",
    "\n",
    "Good luck! �"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60146c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
